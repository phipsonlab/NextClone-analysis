---
title: "Benchmarking performance"
author: "Givanna Putri"
date: "2024-06-14"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Introduction

To address reviewer's comments demanding runtime benchmarking.

What was done?
Run NextClone on both DNAseq and scRNAseq data setting chunks 10-50 at increment of 10.
Duration was measured by checking the performance report generated by Nextflow.
This duration was then manually copied from the html performance report file to the csv
file read in as an input for this analysis.

The following shell script was run to generate the data:


```{r eval=FALSE}
# for scRNAseq
#!/bin/bash

module load nextflow

basedir=/vast/projects/Goel_senescence/nextclone_dev/07_analysis/pilot_dataset/02_run_nextclone

for nchunks in {10..50..10}
do
    outdir=$basedir/output_20240613/nchunk_$nchunks
    nextflow run main.nf \
         --mode scRNAseq \
         --scrnaseq_bam_files $basedir/for_bioinf_first_submission/data/scrnaseq_bam_files \
         --n_chunks $nchunks \
         --publish_dir $outdir \
         -with-report $outdir/report_sc_nchunk"$nchunks".html
done

# for DNAseq
#!/bin/bash

module load nextflow

basedir=/vast/projects/Goel_senescence/nextclone_dev/07_analysis/ngs_v1/run_nextclone/for_bioinf_rebuttal/

datasets=('8k' '10k')

for dat in "${datasets[@]}"
do
    for nchunks in {10..50..10}
    do  
        outdir=$basedir/output_20240321/$dat/nchunk_$nchunks
        # only need to run this once. ran it for testing the loop is ok.
        # echo "Creating $outdir"
        # mkdir -p $outdir
        nextflow run main.nf \
            --mode DNAseq \
            --dnaseq_fastq_files $basedir/data/dnaseq_fastq_files/$dat \
            --n_chunks $nchunks \
            --publish_dir $outdir \
            -with-report $outdir/performance_report
    done
done
```

```{r}
library(data.table)
library(ggplot2)
library(scales)
library(stringr)
```

## Analysis

```{r}
duration_dt <- fread("data/benchmark_duration.csv")
```

Have to convert the duration containing number of hours, minutes, seconds to just minutes.

```{r}
duration_dt$duration_in_hours <- sapply(duration_dt$duration, function(dur) {
    dur_split <- str_split_1(dur, " ")
    hour <- as.numeric(gsub("h", "", dur_split[1]))
    minute <- as.numeric(gsub("m", "", dur_split[2]))
    second <- as.numeric(gsub("s", "", dur_split[3]))
    
    total_duration_in_hour <- hour + (minute / 60) + (second / 3600)
    
    return(total_duration_in_hour)
    
})
```

Descramble the dataset column so we can find what dataset and number of chunk

```{r}
duration_dt$nchunk <- sapply(duration_dt$dataset, function(dat) {
    dat_split <- str_split_1(dat, "_")
    
    for (component in dat_split) {
        if (length(grep("nchunk", component)) > 0) {
            nchunk <- as.numeric(gsub("nchunk", "", component))
            return(nchunk)
        }
    }
})
duration_dt$nchunk <- factor(duration_dt$nchunk, levels = seq(10, 50, 10))

duration_dt$dataset_name <- sapply(duration_dt$dataset, function(dat) {
    dat_split <- str_split_1(dat, "_")
    
    for (component in dat_split) {
        if (component == "sc") {
            return("scRNAseq")
        } else if (component == "8k") {
            return("DNA-seq (dataset A)")
        } else if (component == "10k") {
            return("DNA-seq (dataset B)")
        }
    }
})
```

Plot duration versus chunk as line graph

```{r}
ggplot(duration_dt, aes(x = nchunk, y = duration_in_hours, colour = dataset_name, group = dataset_name)) +
    geom_line(linewidth = 0.5) +
    geom_point(size = 2) + 
    scale_y_continuous(breaks = pretty_breaks(n=10)) +
    theme_classic() +
    labs(x = "Number of FASTA files", y = "Duration (hours)", colour = "Dataset",
         title = "Benchmarking of NextClone Runtime") 
```






















